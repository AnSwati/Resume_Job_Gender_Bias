# Resume_Job_Gender_Bias

As the dataset is skewed (i.e., the classes are imbalanced), and we split the data while maintaining the same percentage for each class, then the class distribution in the test dataset will reflect the same skewness as in the original dataset.


Stratified Sampling: By using stratified sampling, we ensure that each class is proportionally represented in the training, validation, and test sets. This means that if a class constitutes 10% of the original dataset, it will also constitute approximately 10% of each subset created using stratified sampling.

Class Imbalance: If the original dataset has a class imbalance (e.g., 80% class A, 20% class B), the train, validation, and test sets will have a similar class imbalance if stratified sampling is used.

So, when we split your data using stratified sampling, the test set will indeed reflect the same percentage of class distribution as in the original dataset.
\\\\\\\



\\\\\


## README

This project implements an LSTM-based neural network model for multi-class classification of job occupations based on resumes. The dataset includes text data describing skills and job descriptions, which are combined into a single 'CV' column for analysis. The project involves several key steps: data preprocessing (including tokenization and padding of text sequences), encoding of categorical labels, and the use of pre-trained GloVe embeddings to enhance the text representations. The data is split into training, validation, and test sets using stratified sampling to maintain class distribution. A gender-weighted sampling approach is employed to handle class imbalance, ensuring fair representation of genders in each occupation. The LSTM model is then trained and evaluated, with performance metrics such as accuracy, precision, and recall calculated separately for males and females to assess gender bias. The results, including TPR and precision gaps between genders, are documented to provide insights into the model's fairness and effectiveness.


@article{TYAGI2024100283, title = {Enhancing gender equity in resume job matching via debiasing-assisted deep generative model and gender-weighted sampling}, journal = {International Journal of Information Management Data Insights}, volume = {4}, number = {2}, pages = {100283}, year = {2024}, issn = {2667-0968}, doi = {https://doi.org/10.1016/j.jjimei.2024.100283}, url = {https://www.sciencedirect.com/science/article/pii/S2667096824000727}, author = {Swati Tyagi and Anuj and Wei Qian and Jiaheng Xie and Rick Andrews}, keywords = {Supervised learning, Natural language processing, Algorithmic fairness, Gender bias, Online recruiting, Compounding injustices, Automated recruitment, Mitigating bias, Equality of opportunity}, abstract = {Our work aims to mitigate gender bias within word embeddings and investigates the effects of these adjustments on enhancing fairness in resume job-matching problems. By conducting a case study on resume data, we explore the prevalence of gender bias in job categorization—a significant barrier to equal career opportunities, particularly in the context of machine learning applications. This study scrutinizes how biased representations in job assignments, influenced by a variety of factors such as skills and resume descriptors within diverse semantic frameworks, affect the classification process. The investigation extends to the nuanced language of resumes and the presence of subtle gender biases, including the employment of gender-associated terms, and examines how these terms’ vector representations can skew fairness, leading to a disproportionate mapping of resumes to job categories based on gender. Our findings reveal a significant correlation between gender discrepancies in classification true positive rate and gender imbalances across professions that potentially deepen these disparities. The goal of this study is to (1) mitigate bias at the level of word embeddings via a debiasing-assisted deep generative modeling approach, thereby fostering more equitable and gender-fair vector representations; (2) evaluate the resultant impact on the fairness of job classification; (3) explore the implementation of a gender-weighted sampling technique to achieve a more balanced representation of genders across various job categories when such an imbalance exists. This approach involves modifying the data distribution according to gender before it is input into the classifier model, aiming to ensure equal opportunity and promote gender fairness in occupational classifications. The code for this paper is publicly available on GitHub.} }
